{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Leaf Classification - Forked\n## Using Neural Networks through Keras",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "__author__ : Najeeb Khan, Yasir Mir, Zafarullah Mahmood\n\n__team__ : artificial_stuPiDity\n\n__institution__ : Jamia Millia Islamia\n\n__email__ : najeeb.khan96@gmail.com",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Importing standard libraries\n\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Importing sklearn libraries\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder",
   "execution_count": 3,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Keras Libraries for Neural Networks\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation\nfrom keras.utils.np_utils import to_categorical",
   "execution_count": 4,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Set figure size to 20x10\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,10",
   "execution_count": 5,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Read data from the CSV file\n\ndata = pd.read_csv('../input/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\nID = data.pop('id')",
   "execution_count": 6,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data.shape",
   "execution_count": 8,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Since the labels are textual, so we encode them categorically\n\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)",
   "execution_count": 7,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Most of the learning algorithms are prone to feature scaling\n## Standardising the data to give zero mean =)\n\nX = StandardScaler().fit(data).transform(data)\nprint(X.shape)",
   "execution_count": 8,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\n\ny_cat = to_categorical(y)\nprint(y_cat.shape)",
   "execution_count": 9,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Developing a layered model for Neural Networks\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n\nmodel = Sequential()\nmodel.add(Dense(1024,input_dim=192,  init='uniform', activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(99, activation='softmax'))",
   "execution_count": 49,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Error is measured as categorical crossentropy or multiclass logloss\nmodel.compile(loss='categorical_crossentropy',optimizer='adam', metrics = [\"accuracy\"])",
   "execution_count": 50,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Fitting the model on the whole training data\nhistory = model.fit(X,y_cat,batch_size=32,\n                    nb_epoch=400,verbose=0)",
   "execution_count": 51,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#validation:\n\n#scores = pd.DataFrame(history.history)\n#min(scores['val_loss'])\n#scores.loc[20:,[\"loss\", \"val_loss\"]].plot()",
   "execution_count": 52,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test = pd.read_csv('../input/test.csv')",
   "execution_count": 53,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "index = test.pop('id')",
   "execution_count": 54,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test = StandardScaler().fit(test).transform(test)",
   "execution_count": 55,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "yPred = model.predict_proba(test)",
   "execution_count": 56,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Converting the test predictions in a dataframe as depicted by sample submission\n\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))",
   "execution_count": 57,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())",
   "execution_count": 58,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---------\n\nEarlier` we used a 4 layer network but the result came out to be overfitting the test set. We dropped the count of neurones in the network and also restricted the number of layers to 3 so as to keep it simple.\nInstead of submitting each test sample as a one hot vector we submitted each samples as a probabilistic distribution over all the possible outcomes. This \"may\" help reduce the penalty being exercised by the multiclass logloss thus producing low error on the leaderboard! ;)\nAny suggestions are welcome!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  }
 ]
}